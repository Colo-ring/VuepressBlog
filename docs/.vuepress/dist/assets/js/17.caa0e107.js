(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{544:function(t,v,_){"use strict";_.r(v);var a=_(3),r=Object(a.a)({},(function(){var t=this,v=t.$createElement,_=t._self._c||v;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("p",[t._v("有没有办法，能够使用"),_("strong",[t._v("较少的训练样本")]),t._v("来获得"),_("strong",[t._v("性能较好的分类器")]),t._v("呢？"),_("strong",[t._v("主动学习")]),t._v("(Active Learning)为我们提供了这种可能。主动学习通过一定的算法查询"),_("strong",[t._v("最有用的")]),t._v("未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。")]),t._v(" "),_("p",[t._v("样本信息指在训练数据集当中每个样本带给模型训练的信息是不同的，即"),_("strong",[t._v("每个样本为模型训练的贡献有大有小")]),t._v("，它们之间是有差异的。")]),t._v(" "),_("p",[t._v("因此，为了尽可能地减小训练集及标注成本，在机器学习领域中，提出主动学习（active learning）方法，优化分类模型。主动学习(active learning)，指的是这样一种学习方法：")]),t._v(" "),_("p",[t._v("有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，这时候，"),_("strong",[t._v("学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。")])]),t._v(" "),_("p",[t._v("这个筛选过程也就是主动学习主要研究的地方了。")]),t._v(" "),_("h2",{attrs:{id:"一、主动学习的基本思想"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#一、主动学习的基本思想"}},[t._v("#")]),t._v(" 一、主动学习的基本思想")]),t._v(" "),_("p",[_("strong",[t._v("A=(C,Q,S,L,U)")])]),t._v(" "),_("p",[t._v("其中 C 为一组或者一个分类器，L是用于训练已标注的样本。Q 是查询函数，用于从未标注样本池U中查询信息量大的信息，S是督导者，可以为U中样本标注正确的标签。学习者通过少量初始标记样本L开始学习，通过一定的查询函数Q选择出一个或一批最有用的样本，并向督导者询问标签，然后利用获得的新知识来训练分类器和进行下一轮查询。主动学习是一个循环的过程，直至达到某一停止准则为止。")]),t._v(" "),_("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/202111150930987"}}),t._v(" "),_("p",[t._v("那么，"),_("strong",[t._v("什么样的样本是有用的呢？"),_("strong",[t._v("即查询函数查询的是什么样的样本呢？在各种主动学习方法中，查询函数的设计最常用的策略是："),_("strong",[t._v("不确定性")]),t._v("准则（uncertainty）和")]),t._v("差异性")]),t._v("准则（diversity）。")]),t._v(" "),_("p",[t._v("对于不确定性，我们可以借助信息"),_("strong",[t._v("熵")]),t._v("的概念来进行理解。我们知道信息熵是衡量信息量的概念，也是衡量不确定性的概念。信息熵越大，就代表不确定性越大，包含的信息量也就越丰富。事实上，有些基于不确定性的主动学习查询函数就是使用了信息熵来设计的，比如熵值装袋查询（Entropy query-by-bagging）。所以，不确定性策略就是要想方设法地找出不确定性高的样本，因为这些样本所包含的丰富信息量，对我们训练模型来说就是有用的。")]),t._v(" "),_("p",[t._v("那么差异性怎么来理解呢？之前说到或查询函数每次迭代中查询一个或者"),_("strong",[t._v("一批")]),t._v("样本。我们当然希望所查询的样本提供的信息是全面的，各个样本提供的信息不重复不冗余，即样本之间具有一定的差异性。**在每轮迭代抽取单个信息量最大的样本加入训练集的情况下，每一轮迭代中模型都被重新训练，以新获得的知识去参与对样本不确定性的评估可以有效地避免数据冗余。**但是如果每次迭代查询一批样本，那么就应该想办法来保证样本的差异性，避免数据冗余。")]),t._v(" "),_("p",[_("strong",[t._v("下图为Action learning在相同的标注样本数目下与监督学习算法的比较：")])]),t._v(" "),_("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/202111150931067"}}),t._v(" "),_("p",[t._v("从上图也可以看出来，在相同数目的标注数据中，主动学习算法比监督学习算法的分类误差要低。这里注意横轴是标注数据的数目，对于主动学习而言，相同的标注数据下，主动学习的样本数>监督学习，这个对比主要是为了说明两者对于训练样本的使用效率不同：主动学习训练使用的样本都是经过算法筛选出来对于模型训练有帮助的数据，所以效率高。但是如果是相同样本的数量下去对比两者的误差，那肯定是监督学习占优，这是毋庸置疑的。")]),t._v(" "),_("h2",{attrs:{id:"二、active-learning与半监督学习的不同"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#二、active-learning与半监督学习的不同"}},[t._v("#")]),t._v(" 二、Active Learning与半监督学习的不同")]),t._v(" "),_("p",[t._v("很多人认为主动学习也属于半监督学习的范畴了，但实际上是不一样的，半监督学习和直推学习(transductive learning)以及主动学习，都属于利用未标记数据的学习技术，但基本思想还是有区别的。")]),t._v(" "),_("p",[t._v("如上所述，"),_("strong",[t._v("主动学习的“主动”，指的是主动提出标注请求，也就是说，还是需要一个外在的能够对其请求进行标注的实体(通常就是相关领域人员)，即主动学习是交互进行的。")])]),t._v(" "),_("p",[t._v("而半监督学习，特指的是学习算法不需要人工的干预，基于自身对未标记数据加以利用。")]),t._v(" "),_("h2",{attrs:{id:"三、样例选择算法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#三、样例选择算法"}},[t._v("#")]),t._v(" 三、样例选择算法")]),t._v(" "),_("p",[_("strong",[t._v("根据获得未标注样例的方式，可以将主动学习分为两种类型：基于流的和基于池的。")])]),t._v(" "),_("ul",[_("li",[t._v("基于流(stream-based)的主动学习中，未标记的样例按先后顺序逐个提交给选择引擎，由选择引擎决定是否标注当前提交的样例，如果不标注，则将其丢弃。")]),t._v(" "),_("li",[t._v("基于池(pool-based)的主动学习中则维护一个未标注样例的集合，由选择引擎在该集合中选择当前要标注的样例。")])]),t._v(" "),_("h3",{attrs:{id:"基于池的样例选择算法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#基于池的样例选择算法"}},[t._v("#")]),t._v(" 基于池的样例选择算法")]),t._v(" "),_("h4",{attrs:{id:"_1、基于不确定度缩减的方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1、基于不确定度缩减的方法"}},[t._v("#")]),t._v(" 1、基于不确定度缩减的方法")]),t._v(" "),_("p",[t._v("这类方法选择那些当前基准分类器最不能确定其分类的样例进行标注。这类方法以信息熵作为衡量样例所含信息量大小的度量，而信息熵最大的样例正是当前分类器最不能确定其分类的样例。从几何角度看，这种方法优先选择靠近分类边界的样例。")]),t._v(" "),_("h4",{attrs:{id:"_2、基于版本缩减的方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2、基于版本缩减的方法"}},[t._v("#")]),t._v(" 2、基于版本缩减的方法")]),t._v(" "),_("p",[t._v("这类方法选择那些训练后能够最大程度缩减版本空间的样例进行标注。在二值分类问题中，这类方法选择的样例总是差不多平分版本空间。")]),t._v(" "),_("p",[t._v("代表：QBC算法")]),t._v(" "),_("p",[t._v("QBC算法从版本空间中随机选择若干假设构成一个委员会，然后选择委员会中的假设预测分歧最大的样例进行标注。为了优化委员会的构成，可以采用Bagging、AdaBoost等分类器集成算法从版本空间中产生委员会。")]),t._v(" "),_("h4",{attrs:{id:"_3、基于泛化误差缩减的方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3、基于泛化误差缩减的方法"}},[t._v("#")]),t._v(" 3、基于泛化误差缩减的方法")]),t._v(" "),_("p",[t._v("这类方法试图选择那些能够使未来泛化误差最大程度减小的样例。其一般过程为：首先选择一个损失函数用于估计未来错误率，然后将未标注样例集中的每一个样例都分别估计其能给基准分类器带来的误差缩减，选择估计值最大的那个样例进行标注。")]),t._v(" "),_("p",[t._v("这类方法直接针对分类器性能的最终评价指标，但是计算量较大，同时损失函数的精度对性能影响较大。")]),t._v(" "),_("h4",{attrs:{id:"_4、其它方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4、其它方法"}},[t._v("#")]),t._v(" 4、其它方法")]),t._v(" "),_("p",[t._v("COMB算法：组合三种不同的学习器，迅速切换到当前性能最好的学习器从而使选择样例尽可能高效。")]),t._v(" "),_("p",[t._v("多视图主动学习：用于学习问题为多视图学习的情况，选择那些使不同视图的预测分类不一致的样例进行学习。这种方法对于处理高维的主动学习问题非常有效。")]),t._v(" "),_("p",[t._v("预聚类主动学习：预先运行聚类算法预处理，选择样例时优先选择最靠近分类边界的样例和最能代表聚类的样例（即聚类中心）。")]),t._v(" "),_("h3",{attrs:{id:"基于流的样例选择算法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#基于流的样例选择算法"}},[t._v("#")]),t._v(" 基于流的样例选择算法")]),t._v(" "),_("p",[t._v("基于池的算法大多可以通过调整以适应基于流的情况。但由于基于流的算法不能对未标注样例逐一比较，需要对样例的相应评价指标设定阈值，当提交给选择引擎的样例评价指标超过阈值，则进行标注，但这种方法需要针对不同的任务进行调整，所以难以作为一种成熟的方法投入使用。")])])}),[],!1,null,null,null);v.default=r.exports}}]);