(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{539:function(t,a,s){"use strict";s.r(a);var i=s(3),o=Object(i.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"self-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#self-attention"}},[t._v("#")]),t._v(" Self-attention")]),t._v(" "),s("p",[s("strong",[t._v("self-attention的输出是会考虑整个句子的向量。")])]),t._v(" "),s("p",[s("strong",[t._v("self-attention也可以使用多次：")])]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406181951656.png",alt:"image-20210406181951656"}}),t._v(" "),s("p",[s("strong",[s("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("论文：Attention is all you need."),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("self-attention可以是输入也可以是某 一层的输出")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210408202543.jpg",alt:"微信图片_20210408202543"}}),t._v(" "),s("h3",{attrs:{id:"一、如何产生-b-1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、如何产生-b-1"}},[t._v("#")]),t._v(" 一、如何产生$b^1$")]),t._v(" "),s("p",[s("strong",[t._v("$b^1$是sefl-attention考虑每一个向量和$a^1$（包括$a^1$本身）的相关程度的输出")])]),t._v(" "),s("p",[t._v("每一个和$a^1$的关联程度我们用一个数值α表示。self-attention怎么自动决定两个向量的关联性呢？")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210408202708.jpg",alt:"微信图片_20210408202708"}})]),t._v(" "),s("p",[t._v("我们需要一个计算attention的模组，将该两个向量输入，得到一个输出α。有多种方式计算α：")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406192341829.png",alt:"image-20210406192341829"}}),t._v(" "),s("p",[s("strong",[t._v("Transformer中使用的是Dot-product：")])]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("计算每一个向量和向量$a^1$的关联性，最后通过soft-max计算关联性的比重")])])]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406192722025.png",alt:"image-20210406192722025"}}),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Extract information based on attention scores")]),t._v(" "),s("ul",[s("li",[t._v("给$a^1$到$a^4$乘一个新的向量$W^2$得到新的向量$v$")]),t._v(" "),s("li",[t._v("得到的每一个向量$v^i$和$a^i$相乘，并求和，的到$b^1$")])])])]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406193310142.png",alt:"image-20210406193310142"}}),t._v(" "),s("p",[s("strong",[t._v("由此得知，如果$a^1$和$a^2$的相关性较高，则最终计算出的$b^1$和$b^2$的值就会相近。")])]),t._v(" "),s("h3",{attrs:{id:"二、矩阵计算"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、矩阵计算"}},[t._v("#")]),t._v(" 二、矩阵计算")]),t._v(" "),s("p",[t._v("1、计算得出矩阵Q、K、V")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406204716328.png",alt:"image-20210406204716328"}}),t._v(" "),s("p",[t._v("2、$K^T·Q$ 得到矩阵$A$ → (softmax) → $A^`$")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406204827108.png",alt:"image-20210406204827108"}}),t._v(" "),s("p",[t._v("3、$V·A^`$得到输出矩阵$O$")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406204947257.png",alt:"image-20210406204947257"}}),t._v(" "),s("p",[s("strong",[t._v("可以看出，需要学习的参数只有$W^q、W^k、W^v$")])]),t._v(" "),s("h2",{attrs:{id:"multi-head-self-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#multi-head-self-attention"}},[t._v("#")]),t._v(" Multi-head Self-attention")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406205928957.png",alt:"image-20210406205928957"}}),t._v(" "),s("hr"),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406205955923.png",alt:"image-20210406205955923"}}),t._v(" "),s("hr"),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406210047004.png",alt:"image-20210406210047004"}}),t._v(" "),s("h2",{attrs:{id:"positional-encoding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#positional-encoding"}},[t._v("#")]),t._v(" Positional Encoding")]),t._v(" "),s("p",[s("strong",[t._v("1、No position information in self-attention.")])]),t._v(" "),s("p",[t._v("在Self-attention中，少了一个很重要的信息，就是位置的信息，因为在其一系列矩阵运算中，每一个imput出现在sequence的位置是没有差别的。因此对它而言，每一个输入的距离都是一样的。")]),t._v(" "),s("p",[s("strong",[t._v("2、Each position has a unique pisitional vector $e^i$")])]),t._v(" "),s("p",[t._v("为每一个位置设定一个唯一的向量$e^i$，$i$表示位置，直接加到$a^i$，self-attention识别出现在出现的位置在$i$这个位置。")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406210800351.png",alt:"image-20210406210800351"}}),t._v(" "),s("p",[s("strong",[t._v("3、hand-crafted")])]),t._v(" "),s("p",[t._v("$e^i$，即位置的表示方法尚在研究中，可以有不同的表示方法，如下图，每一列表示一个位置向量：")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406211143589.png",alt:"image-20210406211143589"}}),t._v(" "),s("p",[s("strong",[t._v("4、Learn from data")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://arxiv.org/abs/2003.09229",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考文献"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("img",{attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210408202906.jpg",alt:"微信图片_20210408202906"}})]),t._v(" "),s("h2",{attrs:{id:"self-attention的应用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#self-attention的应用"}},[t._v("#")]),t._v(" Self-attention的应用")]),t._v(" "),s("h3",{attrs:{id:"一、nlp"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、nlp"}},[t._v("#")]),t._v(" 一、NLP")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406211708901.png",alt:"image-20210406211708901"}}),t._v(" "),s("h3",{attrs:{id:"二、self-attention-vs-cnn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、self-attention-vs-cnn"}},[t._v("#")]),t._v(" 二、Self-attention vs. CNN")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406213349923.png",alt:"image-20210406213349923"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406213408220.png",alt:"image-20210406213408220"}}),t._v(" "),s("p",[s("strong",[t._v("CNN是简化版的Self-attention")])]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406214155489.png",alt:"image-20210406214155489"}}),t._v(" "),s("p",[t._v("论文："),s("a",{attrs:{href:"https://arxiv.org/abs/1911.03584",target:"_blank",rel:"noopener noreferrer"}},[t._v("On the Relationship between Self-Attention and Convolutional Layers"),s("OutboundLink")],1),t._v("，用数学的方式严谨地讲了CNN就是Self-attention的特例。")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406214522341.png",alt:"image-20210406214522341"}}),t._v(" "),s("h3",{attrs:{id:"二、self-attention-vs-rnn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、self-attention-vs-rnn"}},[t._v("#")]),t._v(" 二、Self-attention vs. RNN")]),t._v(" "),s("p",[s("strong",[t._v("RNN的角色大部分已经可以被Self-attention取代了。")])]),t._v(" "),s("ul",[s("li",[t._v("RNN处理数据时，ssequence中距离较远的两个向量，比较难被考虑。而Self-attention天涯若比邻的机制很好考虑")]),t._v(" "),s("li",[t._v("RNN无法平行处理input、output")])]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406215315873.png",alt:"image-20210406215315873"}}),t._v(" "),s("h3",{attrs:{id:"三、self-attention-for-graph"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、self-attention-for-graph"}},[t._v("#")]),t._v(" 三、Self-attention for Graph")]),t._v(" "),s("p",[t._v("Graph可以看作是一组有关联的向量，因此可以使用self-attention来处理。")]),t._v(" "),s("p",[t._v("Graph不仅有node的信息，还有edge的信息。")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://typora-img-1307960409.cos.ap-shanghai.myqcloud.com/img/image-20210406215848458.png",alt:"image-20210406215848458"}}),t._v(" "),s("p",[s("strong",[t._v("Self-attention在计算Graph的矩阵时，可以只计算有edge相连的node。")])])])}),[],!1,null,null,null);a.default=o.exports}}]);